#!/usr/bin/env python

# Download all images from blogspot.

import json, re, sys
from you_get.common import *
from you_get.util import *

site_info = 'blogspot'

def main():
    for arg in sys.argv[1:]:
        # TODO: use get_content with fake headers
        response = get_response(arg, faker=True)
        page = str(response.data)

        matches = re.findall(r'(https?://blogger\.googleusercontent\.com/[/\=\-\w]+)', page)
        images = []
        for url in matches:
            if len(url.split('=')) > 1:
                if url.split('=')[1][0:3] == 's72':
                    continue
            if url.split('/')[-2][0:3] == 's72':
                continue
            url = url.split('=')[0]
            if url not in images:
                images.append(url)

        i = 1
        for image in images:
            #photo_id = image.split('/')[-1]
            title = '%s' % i
            i += 1

            ext, size = 'jpg', url_size(image)
            print_info(site_info, title, ext, size)
            download_urls(urls=[image],
                          title=title,
                          output_dir='.',
                          total_size=size,
                          ext=ext,
                          merge=False)

    return

if __name__ == '__main__':
    main()
