#!/usr/bin/env python
# Save a tweet in the current directory as pt format (possibly with images).
#
# Dependencies:
# - python ~ 3.5
#   - you-get ~ 0.4
#
# Example:
#   $ Get-Tweet https://twitter.com/sirjoancornella/status/1267444407168630787
#   $ ls
#     1267444407168630787.pt  1267444407168630787_EZbd91UWAAAnuSx.jpg

import json, re, sys
from datetime import datetime
from you_get.common import *
from you_get.util import *
from you_get.extractors.twitter import twitter_download

site_info = 'Twitter'

def get_url(url):
    m = re.match('^https?://(mobile\.)?twitter\.com/([^/]+)/status/(\d+)', url)
    assert m
    screen_name, item_id = m.group(2), m.group(3)
    page_title = "{} [{}]".format(screen_name, item_id)
    log.w(page_title.split('\n')[0].split(': ')[0])

    authorization = 'Bearer AAAAAAAAAAAAAAAAAAAAANRILgAAAAAAnNwIzUejRCOuH5E6I8xnZz4puTs%3D1Zv7ttfk8LF81IUq16cHjhLTvJu4FA33AGWWjCpTnA'

    ga_url = 'https://api.twitter.com/1.1/guest/activate.json'
    ga_content = post_content(ga_url, headers={'authorization': authorization})
    guest_token = json.loads(ga_content)['guest_token']

    api_url = 'https://api.twitter.com/2/timeline/conversation/%s.json?tweet_mode=extended' % item_id
    api_content = get_content(api_url, headers={'authorization': authorization, 'x-guest-token': guest_token})

    info = json.loads(api_content)
    item = info['globalObjects']['tweets'][item_id]
    user_id_str = item['user_id_str']
    author = info['globalObjects']['users'][user_id_str]['name']
    created_at = datetime.strptime(item['created_at'], '%a %b %d %X %z %Y').strftime('%a %d %b %Y')
    full_text = item['full_text']

    outf = open(item_id + '.pt', 'w')
    outf.write('---\n')
    outf.write('author: %s\n' % author)
    outf.write('date: %s\n' % created_at)
    outf.write('source: %s\n' % 'Twitter')
    outf.write('url: %s\n' % url)
    outf.write('---\n')
    outf.write(full_text)
    outf.write('\n')
    outf.close()

    if 'extended_entities' in item:
        media = item['extended_entities']['media']
        output_dir = '.'
        for medium in media:
            if 'video_info' in medium:
                # FIXME: we're assuming one tweet only contains one video here
                variants = medium['video_info']['variants']
                variants = sorted(variants, key=lambda kv: kv.get('bitrate', 0))
                urls = [ variants[-1]['url'] ]
                size = urls_size(urls)
                mime, ext = variants[-1]['content_type'], 'mp4'

                download_urls(urls, page_title, ext, size, output_dir)

            else:
                title = item_id + '_' + medium['media_url_https'].split('.')[-2].split('/')[-1]
                urls = [ medium['media_url_https'] + ':orig' ]
                size = urls_size(urls)
                ext = medium['media_url_https'].split('.')[-1]

                download_urls(urls, title, ext, size, output_dir)

    if 'quoted_status_permalink' in item:
        quoted_url = item['quoted_status_permalink']['expanded']
        log.w('>>> QUOTED: %s' % quoted_url)
        get_url(quoted_url)


def main():
    for url in sys.argv[1:]:
        get_url(url)

if __name__ == '__main__':
    main()
